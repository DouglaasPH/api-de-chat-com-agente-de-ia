# ğŸ“¡ API de Chat com Agente de IA (FastAPI + Strands Agents + Ollama)

Este projeto implementa uma API de chat integrada a um Agente de IA capaz de responder perguntas gerais e executar cÃ¡lculos usando uma _Math Tool_.
A aplicaÃ§Ã£o utiliza **FastAPI**, **Strands Agents SDK** e o modelo **llama3.1** rodando localmente via **Ollama**.

---

## ğŸ“ CÃ³digo-Fonte

O repositÃ³rio contÃ©m:

```
api/
    routes/
        math.py              â†’ Endpoint "/chat" da API para operaÃ§Ãµes matemÃ¡ticas
    schemas/
        math_message.py      â†’ Modelos Pydantic para validar request
application/
    agent_service.py         â†’ LÃ³gica do agente de IA (gerencia criaÃ§Ã£o do agente, configuraÃ§Ã£o do modelo e execuÃ§Ã£o das mensagens)
    math_tool.py             â†’ Ferramenta usada pelo agente para cÃ¡lculos
domain/
    math_service.py          â†’ Regras de negÃ³cio das operaÃ§Ãµes matemÃ¡ticas
infrastructure/
    llm_ollama.py            â†’ IntegraÃ§Ã£o com o provedor Ollama (cria e configura o modelo de IA usando variÃ¡veis do .env)
    settings.py              â†’ Carrega e gerencia variÃ¡veis de ambiente usando Pydantic Settings
.env.example                 â†’ Exemplo das variÃ¡veis de ambiente necessÃ¡rias
.gitignore                   â†’ Arquivos e pastas ignorados pelo Git
main.py                      â†’ Entrada principal da API (FastAPI)
pyproject.toml               â†’ ConfiguraÃ§Ã£o do Poetry e dependÃªncias
poetry.lock                  â†’ VersÃµes exatas das dependÃªncias instaladas
README.md                    â†’ InstruÃ§Ãµes para instalar, rodar e usar o projeto

```

---

# ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

## 1. ğŸ”§ PrÃ©-requisitos

- **Python â‰¥ 3.14**
- Ollama instalado
- Modelo local **llama3.1**

### Instalar Ollama

Baixe em: [https://ollama.com/download](https://ollama.com/download)

Inicie o servidor:

```sh
ollama serve
```

Baixe o modelo exigido:

```sh
ollama pull llama3.1
```

---

## 2. ğŸ“¦ Instalar dependÃªncias

### Usando Poetry

```sh
poetry install
```

---

## 3. ğŸ” Configurar variÃ¡veis de ambiente

O projeto inclui um arquivo **`.env.example`** com as configuraÃ§Ãµes iniciais necessÃ¡rias.
Basta copiÃ¡-lo e renomear para **`.env`**:

```sh
cp .env.example .env
```

---

## 4. â–¶ï¸ Executar o servidor FastAPI

### Com uvicorn:

```sh
uvicorn main:app --host 0.0.0.0 --port 8000
```

### Com Poetry:

```sh
poetry run uvicorn main:app --host 0.0.0.0 --port 8000
```

Acesse a API:

```
http://localhost:8000
```

Swagger UI:

```
http://localhost:8000/docs
```

---

# ğŸ’¬ Endpoint de Chat

### POST `/chat`

ğŸ“¤ Exemplo de requisiÃ§Ã£o:

```json
{
  "message": "Quanto Ã© 12 * 19?"
}
```

ğŸ“¥ Exemplo de resposta:

```json
{
  "response": "A resposta para a pergunta Ã©: 228."
}
```

---

# ğŸ§  Funcionamento do Agente de IA

- Perguntas gerais â†’ respondidas pelo modelo **llama3.1**
- Perguntas matemÃ¡ticas â†’ encaminhadas automaticamente para a _Math Tool_
- A tool resolve:

  - Soma
  - SubtraÃ§Ã£o
  - MultiplicaÃ§Ã£o
  - DivisÃ£o
  - Raiz quadrada
  - PotenciaÃ§Ã£o

---

# ğŸ“„ .gitignore

Inclui:

```
.env
*.pyc
**/__pycache__/
.venv/
.cache/
```

---

# âœ… Requisitos do Case Atendidos

âœ” Python â‰¥ 3.14  
âœ” API FastAPI com POST `/chat`  
âœ” Modelos e agentes configurados via `.env`  
âœ” Strands Agents SDK integrado  
âœ” Ferramenta matemÃ¡tica funcionando  
âœ” ExecuÃ§Ã£o local com Ollama + llama3.1  
âœ” CÃ³digo limpo e organizado
